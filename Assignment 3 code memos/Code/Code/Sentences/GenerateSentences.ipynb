{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bfd61a1-448a-4e9f-bb26-9b640d4f9795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Column1    ciluba     french  score sentiment            nature  \\\n",
      "0           0     Akula      parle    2.0   Positif              Verb   \n",
      "1           1     Aluja      remet    3.0   Positif              Verb   \n",
      "2           2  Andamuna     repond    9.0   Positif              Verb   \n",
      "3           3    Angata      prend    9.0   Positif              Verb   \n",
      "4           4   Bilamba     habits    8.0   Positif              Word   \n",
      "...       ...       ...        ...    ...       ...               ...   \n",
      "1848     1851      Lala       dors    4.0   Positif              Verb   \n",
      "1849     1852       Anu  seulement    3.0   Positif            Adverb   \n",
      "1850     1853      yeya         il    0.0    Neutre  personal pronoun   \n",
      "1851     1854      tetu       nous    0.0    Neutre  personal pronoun   \n",
      "1852     1855     Tshia         de    0.0    Neutre           Article   \n",
      "\n",
      "         English  Afrikaans           Zulu          Xhosa  \n",
      "0          Speak      Praat        Khuluma         Thetha  \n",
      "1     Hands over  Hande oor  Izandla phezu  Izandla phezu  \n",
      "2        Answers  Antwoorde    Izimpendulo     Iimpendulo  \n",
      "3          Takes       Neem       Kuthatha        Ithatha  \n",
      "4        Clothes      Klere       Izingubo       Iimpahla  \n",
      "...          ...        ...            ...            ...  \n",
      "1848       Sleep      Slaap           Lala           Lala  \n",
      "1849        Only      Slegs        Kuphela        Kuphela  \n",
      "1850          he         hy           yena           yena  \n",
      "1851          We        Ons          Thina          Thina  \n",
      "1852          of        van            kwe             ye  \n",
      "\n",
      "[1853 rows x 10 columns]\n",
      "Null Values 0\n",
      "Null Values 0\n",
      "Null Values 0\n",
      "Empty DataFrame\n",
      "Columns: [ciluba, french, score, sentiment, nature, English, Afrikaans, Zulu, Xhosa]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [ciluba, french, score, sentiment, nature, English, Afrikaans, Zulu, Xhosa]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [ciluba, french, score, sentiment, nature, English, Afrikaans, Zulu, Xhosa]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [ciluba, french, score, sentiment, nature, English, Afrikaans, Zulu, Xhosa]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [ciluba, french, score, sentiment, nature, English, Afrikaans, Zulu, Xhosa]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [ciluba, french, score, sentiment, nature, English, Afrikaans, Zulu, Xhosa]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('LexiconPOSClean.csv',sep=\";\") # This one has duplicates\n",
    "print(df)\n",
    "df = df.drop('Column1', axis=1)\n",
    "if df['Afrikaans'].isna().sum() > 0:\n",
    "    raise Exception(\"Sorry, no numbers below zero\")\n",
    "\n",
    "print(\"Null Values\",df['Afrikaans'].isna().sum()) \n",
    "print(\"Null Values\",df['Zulu'].isna().sum())\n",
    "print(\"Null Values\",df['Xhosa'].isna().sum()) \n",
    "#Remove value not formatting correctly (Try Download again and see?)\n",
    "def DropRows(language):\n",
    "    list =  df[df[language].isnull()].index.tolist()\n",
    "    for index in list:\n",
    "        df.drop(index,inplace=True)\n",
    "\n",
    "DropRows('Afrikaans')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "languages = ['ciluba', 'french','English','Afrikaans','Zulu','Xhosa']\n",
    "\n",
    "def DisplayNumbers():\n",
    "    for lang in languages:\n",
    "        print(df[df[lang].str.contains(r'\\d+')])\n",
    "DisplayNumbers() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5fa37ac-f493-46f0-9845-5d9a647bd036",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "['Word', 'Verb', 'Number', 'personal pronoun', 'Adverb', 'Adjective', 'Article', 'Adjetive']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "from keytotext import pipeline\n",
    "nlp = pipeline(\"k2t\")\n",
    "import random\n",
    "\n",
    "print(df['nature'].value_counts().index.tolist())\n",
    "nouns = df.loc[df['nature'] == 'Word']\n",
    "verbs = df.loc[df['nature'] == 'Verb']\n",
    "pronouns = df.loc[df['nature'] == 'personal pronoun']\n",
    "\n",
    "try:\n",
    "    df_sentences\n",
    "except NameError:\n",
    "    df_sentences = pd.DataFrame(columns=['English','KeywordVerb','KeywordNoun'])\n",
    "#print(pronouns)\n",
    "\n",
    "def ListOfWords(num):\n",
    "    verb = verbs.sample()\n",
    "    noun = nouns.sample()\n",
    "    pronoun = pronouns.sample()\n",
    "    \n",
    "        #https://stackoverflow.com/questions/16729574/how-can-i-get-a-value-from-a-cell-of-a-dataframe\n",
    "    verb = verb['English'].values[0]\n",
    "    noun = noun['English'].values[0]\n",
    "    pronoun = pronoun['English'].values[0]\n",
    "        #print(\"Verb is\",verb)\n",
    "        #print(\"noun is\",noun)\n",
    "\n",
    "        # verb = random.sample(verbs, 1)\n",
    "        # noun = random.sample(nouns,1)\n",
    "        # Remove words with two value from list of verbs to be selected... and everything else.\n",
    "#print(\"pronoun is\",pronoun)\n",
    "    sentence = nlp([verb,noun])\n",
    "        #print(sentence)\n",
    "        #print(type(sentence))\n",
    "#def GenerateSentences():\n",
    "#https://stackoverflow.com/questions/58773880/str-contains-pandas-returns-str-object-has-no-attribute-contains\n",
    "    if '|'  not in sentence:\n",
    "        splitList = sentence.split(\" \")\n",
    "            #print(len(splitList))\n",
    "            #print(\"String is\",sentence)\n",
    "        if len(splitList) < 8:\n",
    "            print(\"added sentence\")\n",
    "            df_sentences.loc[len(df_sentences), 'English'] = sentence\n",
    "            df_sentences.loc[len(df_sentences)-1, 'KeywordVerb'] = verb # Try get length of column\n",
    "            df_sentences.loc[len(df_sentences)-1, 'KeywordNoun'] = noun\n",
    "            #print(df_sentences['English'])\n",
    "    if len(df_sentences) < num:\n",
    "        #print(\"rerun\")\n",
    "        ListOfWords(num)\n",
    "                #df_sentences.concat([{'English':sentence}], ignore_index=True)\n",
    "                #list.append(sentence)\n",
    "                #print('beeeeeeh!')\n",
    "            #if noun and verb  in sentence:\n",
    "                #list.append(sentence)\n",
    "               # print('beeeeeeh!')\n",
    "        \n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b83cec3-dcc9-4174-973a-927e06bd3945",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del df_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f25988f-5118-4d92-9360-36e749720bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n",
      "added sentence\n"
     ]
    }
   ],
   "source": [
    "ListOfWords(200) # Put in data paramaeter here to put in range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "925512b2-d4d1-470b-bcad-48d9dd4a9279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>KeywordVerb</th>\n",
       "      <th>KeywordNoun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sieve's 'harm'.</td>\n",
       "      <td>Sieve</td>\n",
       "      <td>Hazard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anger is a celebrated song.</td>\n",
       "      <td>Celebrate</td>\n",
       "      <td>Anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>To wipe's road is a road.</td>\n",
       "      <td>To wipe</td>\n",
       "      <td>road</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Teach is a joy.</td>\n",
       "      <td>Teach</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Come's pan is a dish.</td>\n",
       "      <td>come</td>\n",
       "      <td>pan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Feet is a tooth.</td>\n",
       "      <td>Feet</td>\n",
       "      <td>tooth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>To rent out Sweat is a restaurant.</td>\n",
       "      <td>to rent out</td>\n",
       "      <td>Sweat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>The prayer for a mother is \"belonged\".</td>\n",
       "      <td>pray</td>\n",
       "      <td>mother</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>Pride is a lie.</td>\n",
       "      <td>lie</td>\n",
       "      <td>Pride</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Say's holiness.</td>\n",
       "      <td>Say</td>\n",
       "      <td>holiness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    English  KeywordVerb KeywordNoun\n",
       "0                           Sieve's 'harm'.        Sieve      Hazard\n",
       "1               Anger is a celebrated song.    Celebrate       Anger\n",
       "2                 To wipe's road is a road.      To wipe        road\n",
       "3                           Teach is a joy.        Teach         joy\n",
       "4                     Come's pan is a dish.         come         pan\n",
       "..                                      ...          ...         ...\n",
       "195                        Feet is a tooth.         Feet       tooth\n",
       "196      To rent out Sweat is a restaurant.  to rent out       Sweat\n",
       "197  The prayer for a mother is \"belonged\".         pray      mother\n",
       "198                         Pride is a lie.          lie       Pride\n",
       "199                         Say's holiness.          Say    holiness\n",
       "\n",
       "[200 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6f89ebe0-b9e9-4bc7-91ce-ea08139e1d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One of the main ingredients of work in the industry is the work of clothing.', \"The parent is the responsibility for the dish 'Areport'.\"]\n"
     ]
    }
   ],
   "source": [
    "print(list)\n",
    "https://stackoverflow.com/questions/26666919/add-column-in-dataframe-from-list\n",
    "df['new_col'] = mylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a78ea37d-c25a-40cb-8aa2-332cffa6d354",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences.to_csv('GeneratedSentences.csv',encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4418a5bb-8198-4927-8699-a934aad41b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We|Tach|Journal\" is a joy.\n"
     ]
    }
   ],
   "source": [
    "sentence = nlp(['We','Teach','joy'])\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1d42eb84-b440-425b-9a24-9a1f6bdb4fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('FinalLexicon.csv',encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da186969-b784-436d-aa23-7f2dbebb75bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\magai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\magai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\magai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[('run', 'VB')]\n",
      "0\n",
      "Speak\n",
      "[('Speak', 'NOUN'), ('is', 'VERB'), ('a', 'DET'), ('language', 'NOUN'), ('spoken', 'VERB'), ('in', 'ADP'), ('Speak', 'NOUN'), ('.', '.')]\n",
      "<class 'str'>\n",
      "1\n",
      "Hands over\n",
      "[('Hands', 'NOUN'), ('over', 'PRT'), ('is', 'VERB'), ('a', 'DET'), ('hands', 'NOUN'), ('over', 'ADP'), ('.', '.')]\n",
      "<class 'str'>\n",
      "2\n",
      "Answers\n",
      "[('Answers', 'NOUN'), ('is', 'VERB'), ('a', 'DET'), ('question', 'NOUN'), ('.', '.')]\n",
      "<class 'str'>\n",
      "3\n",
      "Takes\n",
      "[('Takes', 'NOUN'), ('is', 'VERB'), ('a', 'DET'), ('take', 'NOUN'), ('.', '.')]\n",
      "<class 'str'>\n",
      "4\n",
      "Clothes\n",
      "[('Clothes', 'NOUN'), ('is', 'VERB'), ('a', 'DET'), ('place', 'NOUN'), ('to', 'PRT'), ('eat', 'VERB'), ('burgers', 'NOUN'), ('.', '.')]\n",
      "<class 'str'>\n",
      "5\n",
      "Joke\n",
      "[('Joke', 'NOUN'), ('is', 'VERB'), ('a', 'DET'), ('Joke', 'NOUN'), ('player', 'NOUN'), ('.', '.')]\n",
      "<class 'str'>\n",
      "6\n",
      "Tears\n",
      "[('Tears', 'NOUN'), ('is', 'VERB'), ('a', 'DET'), ('tears', 'ADJ'), ('dish', 'NOUN'), ('.', '.')]\n",
      "<class 'str'>\n",
      "7\n",
      "Between\n",
      "[('Between', 'NOUN'), ('two', 'NUM'), ('of', 'ADP'), ('the', 'DET'), ('main', 'ADJ'), ('ingredients', 'NOUN'), ('is', 'VERB'), ('between', 'ADP'), ('.', '.')]\n",
      "<class 'str'>\n",
      "8\n",
      "Light\n",
      "[('Light', 'NOUN'), ('is', 'VERB'), ('a', 'DET'), ('light', 'ADJ'), ('source', 'NOUN'), ('.', '.')]\n",
      "<class 'str'>\n",
      "9\n",
      "Earth\n",
      "[('Earth', 'NOUN'), ('is', 'VERB'), ('the', 'DET'), ('name', 'NOUN'), ('of', 'ADP'), ('Earth', 'NOUN'), (\"'s\", 'PRT'), ('surface', 'NOUN'), ('.', '.')]\n",
      "<class 'str'>\n",
      "Please choose correct answer\n"
     ]
    }
   ],
   "source": [
    "# Visualize data again....\n",
    "\n",
    "# Check Grammar...\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('universal_tagset')\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from datasets import load_metric\n",
    "from keytotext import pipeline\n",
    "nlp = pipeline(\"k2t\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#https://stackoverflow.com/questions/29397708/tagging-a-single-word-with-the-nltk-pos-tagger-tags-each-letter-instead-of-the-w\n",
    "tagged = nltk.pos_tag([\"run\"])\n",
    "english = df['English']\n",
    "nature = df['nature']\n",
    "#df['CorrectGrammar']\n",
    "#https://www.nltk.org/api/nltk.tag.pos_tag.html\n",
    "def GrammarCheck():\n",
    "    list = []\n",
    "    for i,word in enumerate(english[:10]):\n",
    "        print(i)\n",
    "        print(word)\n",
    "        sentence = nlp([word])\n",
    "        # tokenize then find word.\n",
    "        tokens = word_tokenize(sentence)\n",
    "        tagged = nltk.pos_tag(tokens,tagset='universal')\n",
    "        print(tagged)\n",
    "        print(type(nature[i]))\n",
    "        # if tagged == 'VERB' & nature[i] == 'Verb':\n",
    "        #     print(\"bike is Hero\")\n",
    "        # else:\n",
    "        #     list.append(i)\n",
    "        # if tagged == 'NOUN' & nature[i] == \"Word\":\n",
    "        #     print(\"bike is Hero\")\n",
    "        # else:\n",
    "        #     list.append(i)\n",
    "        # if nature[i] != \"Word\" | nature[i] == \"Verb\":\n",
    "        #     list.append(i)\n",
    "\n",
    "\n",
    "    print(\"Please choose correct answer\")\n",
    "print(list)\n",
    "print(tagged)\n",
    "GrammarCheck()\n",
    "\n",
    "# Remove columns from df which are flagged to temporary df with flag and temp df without.\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bc83c4-e89d-46d0-ba0c-e43c2a51a3a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
